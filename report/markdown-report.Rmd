---
title: "Did we already cross Earth’s boundaries without realising it?"
author: "Robert de Ridder"
date: "2025-12-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For years, we’ve been told that global warming must not exceed two degrees Celsius.
The range between 1.5 °C and 2 °C is printed into our collective memory.

Whenever you watch the news, read articles, or consume any kind of climate‑related information,
you’re almost guaranteed to encounter those numbers 1.5 and 2 as if they are 
sacred thresholds. And of course, they must be meaningful; countless scientists 
have worked on them.

Yet I have my doubts. What I see happening in the world, wars, fires, droughts, 
you name it, it doesn’t seem to match those numbers.

Right now, I’m studying data science, and I want to use this opportunity to 
investigate these doubts. While exploring online data sets, I found two particularly 
interesting ones: the temperature anomaly dataset from Our World in Data, 
and the monthly_in_situ_co2_mlo dataset from Scripps CO₂. 
They are continuously updated and form a solid starting point for my investigation.

During this “journey,” I will estimate Earth’s ability to absorb carbon dioxide,
take a closer look at the airborne fraction, and separate temperature change 
into a fast and a slow component.
This may sound abstract, so what does it actually mean?
The fast and slow components refer to how heat is distributed. 
As the planet warms, part of that heat is immediately noticeable at the surface,
while another part is absorbed by the deep oceans. Without oceans, 
the world would be far hotter.

The airborne fraction is the percentage of human‑emitted CO₂ that remains
in the atmosphere. It’s useful when calculating gross and net emissions.
Currently, this fraction is roughly 47%.

The next part of my work becomes quite technical and involves some mathematics,
probably boring to write about and even more boring to read. 
So I used R Markdown to visualize what I’ve done.

In R-studio, I used the following library's:

```{r loading-libs, message=FALSE, warning=FALSE}
library(lubridate)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(readr)
library(patchwork)
library(tidyr)
```
We load the data

```{r, loading-data, warning=FALSE, message=FALSE, out.width="100%"}
load("../rdas/co2_concentration.rda")
load("../rdas/temp_anomaly.rda")
load("../rdas/trend.rda")
load("../rdas/temp_reduct.rda")
load("../rdas/calc_temp_con.rda")
load("../rdas/temp_relaxation.rda")
```
First we read the downloaded files, wrangle it and extract the necessary data.

```{r, extract-data, warning=FALSE, message=FALSE, out.width="100%"}

#-------------------------------------------------------------------------------
# Select required columns and rename column names
temp_anomaly <- read.csv("../data/temperature-anomaly.csv")

temp_anomaly <- temp_anomaly |>
  filter(Entity == "World") |>
  select(
    Year, near_surface_temperature_anomaly,
    near_surface_temperature_anomaly_lower,
    near_surface_temperature_anomaly_upper
  )


# Replace column names
colnames(temp_anomaly) <- c(
  "year",
  "global_average_rise",
  "lower_limit",
  "upper_limit"
)

#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# Adjust monthly_in_situ_co2_mlo.csv dataset:
# remove header etc. and select only year and co2_seas_adj_filled

# 1. Read file as raw text
raw <- readLines("../data/monthly_in_situ_co2_mlo.csv")

# 2. Find first row containing a year (4 digits)
start <- grep("^\\s*[0-9]{4},", raw)[1]

# 3. Read data starting from that row
df <- read_csv(
  "../data/monthly_in_situ_co2_mlo.csv",
  skip = start - 1,
  col_names = FALSE,
  show_col_types = FALSE
)

# 4. Create new column names
names(df) <- c(
  "year", "month", "date_excel", "date_fraction",
  "co2", "co2_seas_adj", "co2_fit", "co2_fit_seas_adj",
  "co2_filled", "co2_seas_adj_filled", "station"
)

# Replace missing values (-99.99) with NA
df <- df |>
  mutate(across(where(is.numeric), ~ ifelse(.x == -99.99, NA, .x)))

# Remove NA values and select only year and co2_seas_adj_filled
co2_concentration <- df |>
  filter(!is.na(co2_seas_adj_filled)) |>
  select(year, co2_seas_adj_filled)

# Normalized CO2 and temperature trend
trend <- co2_concentration |>
  select(year, co2_seas_adj_filled) |>
  left_join(temp_anomaly |> select(year, global_average_rise), by = "year") |>
  mutate(
    co2_norm = scales::rescale(co2_seas_adj_filled),
    temp_norm = scales::rescale(global_average_rise)
  )
today <- Sys.Date()
present_year <- year(today)
previous_year <- present_year - 1

airborne_farction <- 0.47

temp_present <- trend |> filter(year == present_year) |> select(global_average_rise)
avg_temp_present <- mean(temp_present$global_average_rise, na.rm = TRUE)

ppm_last_year <- trend |> filter(year == previous_year) |> select(co2_seas_adj_filled)
avg_ppm_last_year <- mean(ppm_last_year$co2_seas_adj_filled, na.rm = TRUE)

ppm_now <- trend |> filter(year == present_year) |> select(co2_seas_adj_filled)
avg_ppm_now <- mean(ppm_now$co2_seas_adj_filled, na.rm = TRUE)

net_emis <- avg_ppm_now - avg_ppm_last_year

gross_emis <- net_emis / airborne_farction

current_absorbtion <- gross_emis - net_emis
#-------------------------------------------------------------------------------
```
We create models and do some calculations.

```{r, calc-data, warning=FALSE, message=FALSE, out.width="100%"}
#-------------------------------------------------------------------------------
# Sigmoid model

# Sigmoid form: R(T) = R_max / (1 + exp(k * (T - T_mid)))

# Choose initial shape parameters (can be tuned later)
T_mid <- 1.7    # temperature where the curve is roughly halfway down
k     <- 10     # steepness of the transition

# Derive R_max so that R(T_now) = R_now
R_max <- current_absorbtion * (1 + exp(k * (avg_temp_present - T_mid)))

R_log <- function(T) {
  R_max / (1 + exp(k * (T - T_mid)))
}

# Temperature range
T_seq <- seq(0, 2.5, by = 0.01)
R_seq <- R_log(T_seq)

# Put your data in a data frame (ggplot always works with data frames)
temp_reduct <- data.frame(
  T = T_seq,
  R = R_seq
)

#-------------------------------------------------------------------------------

summary <- trend |>
  group_by(year) |>
  summarise(
    mean_value = mean(co2_seas_adj_filled, na.rm = TRUE),
    mean_rise  = mean(global_average_rise, na.rm = TRUE)
  )
co2 <- summary$mean_value

# create a function that follows the global average temp in relation to co2 emission.
model_log <- lm(global_average_rise ~ log(co2_seas_adj_filled), data = trend)

b0 <- coef(model_log)[1]
b1 <- coef(model_log)[2]

global_average <- function(co2) {
  b0 + b1 * log(co2)
}
calc_temp_rise <- global_average(co2)

calc_temp_con <- data.frame(
  calc_temp_rise,
  co2
)

tau_fast <- 4
tau_slow <- 400
C <- avg_ppm_now
w <- 0.7
years <- temp_anomaly$year
T_fast <- temp_anomaly$global_average_rise
T_slow <- temp_anomaly$global_average_rise

# 3. Equilibrium temperature from log(C)
Y <- b0 + b1 * log(C)

# 4. Update fast and slow components
Temp_fast <- T_fast + (Y - T_fast) / tau_fast
Temp_slow <- T_slow + (Y - T_slow) / tau_slow

# 5. Weighted total temperature
temp_total <- w  * Temp_fast + (1 - w) * Temp_slow

temp_relaxation <- data.frame(
  year = years,
  Total_temp = temp_total,
  Land_surface_temp  = Temp_fast,
  Deep_ocean_temp  = Temp_slow
) |>
  pivot_longer(cols = c(Total_temp, Land_surface_temp, Deep_ocean_temp),
               names_to = "component",
               values_to = "temperature")
```
Now we plot the data.

```{r, plot-data, warning=FALSE, message=FALSE, out.width="100%"}
p1 <- ggplot() +
  # model curve
  geom_line(
    data = calc_temp_con,
    aes(x = co2, y = calc_temp_rise),
    linewidth = 0.7,
    color = "steelblue"
  ) +
  
  # real-world smooth
  geom_smooth(
    data = summary,
    aes(x = mean_value, y = mean_rise),
    linewidth = 0.7,
    color = "darkred",
    se = TRUE
  ) +
  geom_hline(yintercept = 1.2,
             color = "darkgreen", linetype = "dashed") +
  
  labs(
    x = "CO₂ concentration",
    y = "Temperature rise (°C)",
    title = "Calculated vs Real Temperature Rise"
  ) +
  theme_economist(base_size = 14)

p2 <- ggplot(temp_reduct, aes(x = T, y = R)) +
  geom_line(linewidth = 0.7) +
  geom_vline(xintercept = 1.2,
             color = "darkgreen", linetype = "dashed") +
  annotate("point",
           x = avg_temp_present,
           y = current_absorbtion,
           color = "darkgreen",
           size = 3) +
  geom_vline(xintercept = 2,
             color = "red", linetype = "dashed") +
  labs(
    x = "Temperature increase (°C above pre-industrial)",
    y = "Net CO₂ uptake (ppm/year)",
    title = "Logistic (S-shaped) model for CO₂ uptake"
  ) +
  theme_economist(base_size = 14)
p3 <- ggplot(temp_relaxation, aes(x = year, y = temperature, color = component)) +
  geom_line(size = 0.7) +
  scale_color_manual(values = c(
    Total_temp = "black",
    Land_surface_temp  = "red",
    Deep_ocean_temp  = "blue"
  )) +
  geom_hline(yintercept = 1.2,
             color = "darkgreen", linetype = "dashed") +
  labs(
    title = "Fast and slow components of total temperature",
    x = "Year",
    y = "Temperature (°C)",
    color = ""
  ) +
  theme_economist(base_size = 14)
```
After a lot of copying, tweaking values, deleting and rewriting code, I finally arrived at something that made sense to me. But the result was also quite shocking. I repeatedly encountered the value 1.2.

How could that be? The media told me under 1.5 °C, we should be in the safe zone.
I ran my code many times, checked it with AI tools, read numerous articles, and surprisingly, the value I found seemed quite realistic, and disturbingly consistent with what we’re seeing in the world today.

They say, pictures say more than a thousand words. To illustrate this, consider the following figure.

```{r, result, warning=FALSE, message=FALSE, out.width="100%"}
cat("Model: global_average =", round(b0, 3), "+", round(b1, 3), "* log(CO2)\n")
cat("Temperature rise now :", avg_temp_present, "(°C)", "\n")
cat("Net CO2 emission :", net_emis, "(ppm)", "\n")
cat("Gross CO2 emission :", gross_emis, "(ppm)", "\n")
cat("Current CO2 absorption :", current_absorbtion, "(ppm)", "\n")
```
And we plot the graph.

```{r, plot, fig.width=10, fig.height=6}
p1 / p2 + p3
```

Lets have a look at the top graph, on the x-axis we have the CO₂ concentration and on the y-axis the temperature increase. The blue line represents the calculated temperature increase as CO₂ is rising. The red line is the real measured temperature. Surprisingly, both match pretty well, except in the last part, it looks like the real temperature is climbing while the calculated temperature steadily increases.

This raised an important question, I had to look further. I know that the earth has an ability to take up some of this CO₂. That’s why I made the second graph.

It shows how the uptake declines as temperatures rise.  And again, 1.2 degrees. Around this temperature, we see that the earth’s ability to take up CO₂ goes down. The little green dot, is the last real measured global average temperature rise. It’s around 1.55 °C. The red dotted line, represents the 2 °C threshold, but around that temperature, the earth’s uptake is almost zero. So,  we’ve been told that global warming must not exceed two degrees Celsius, but this seems a little bit too late. Not everything that seems, is always true. So I looked a little bit further.

In the last graph, I made a two-box simplification. I broke up the global temperature into two components. A fast and a slow component. It represents the land surface temperature (fast), and the deep ocean temperature (slow). For a long time these lines were nicely separated. Meaning that the oceans took up a reasonable portion of the total temperature. Shockingly, we see that around the 1.2 degrees all the lines merge.  For me it’s clear that those numbers 1.5 and 2 as if they are sacred thresholds, don’t hold up any more. This does not mean that 1.2 °C is a precise physical tipping point, but the fact that multiple independent diagnostics converge around this value is noteworthy.

I know my model is very simplified, I looked around on the internet to check if my findings have any similarities with other studies.

For convenience, I made one extra graph that summarises all and indicates the 1.2 °C with a green dotted line.

Conclusion:

The results of my model align remarkably well with recent climate reports. Both the WMO and Copernicus confirm that global temperatures in recent years have hovered around 1.2–1.3 °C above pre‑industrial levels, with 2024 becoming the first calendar year to temporarily exceed 1.5 °C.

Studies show that both land‑based carbon sinks and ocean CO₂ uptake weakened significantly in 2023–2024 due to extreme heat, drought, and record‑warm oceans. At the same time, analyses from 2024/2025 point to a possible phase of accelerated warming, driven in part by declining aerosols and melting ice.

Reports from 2025 highlight that the first tipping points may already occur around 1.2 °C, such as the large‑scale loss of warm‑water coral reefs. The extreme weather events we are already experiencing heatwaves, floods, and crop failures were previously expected only at 1.5–2.0 °C.

In short: the danger zone is beginning earlier than expected. What was once considered “extreme” at 2 °C is now occurring at 1.2–1.5 °C.

